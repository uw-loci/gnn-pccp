{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df9abff-b363-48fd-8cd4-873ce5a7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join, basename, splitext\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "from skimage import img_as_float, exposure\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, roc_curve\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch_geometric.data import Data\n",
    "from models import GraphNet, Preprocess, AttentionPool\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb892be-fc61-43b4-befb-80b56c5f9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_points(pos, edge_index=None, index=None, edge_weights=None, node_weights=None, show=True, img=None, patch_size=1, fig_text=None, box_color='palegreen'):\n",
    "    fig = plt.figure(figsize=(torch.max(pos, 0)[0].numpy()[0], torch.max(pos, 0)[0].numpy()[1]))\n",
    "        # fig_w, fig_h = plt.gcf().get_size_inches()\n",
    "    if edge_index is not None:\n",
    "        if edge_weights is None:\n",
    "            for (src, dst) in edge_index.t().tolist():\n",
    "                src = pos[src].tolist()\n",
    "                dst = pos[dst].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=3, color='royalblue')\n",
    "        else:\n",
    "            i = 0\n",
    "            for (s, d) in edge_index.t().tolist():\n",
    "                src = pos[s].tolist()\n",
    "                dst = pos[d].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=widths[i]*3, color='royalblue')\n",
    "                i+=1\n",
    "    if index is None:\n",
    "        if node_weights is not None:\n",
    "            for p, w in zip(pos, node_weights):\n",
    "                plt.scatter(p[0]*patch_size+int(patch_size/2), p[1]*patch_size+int(patch_size/2), s=w*500, zorder=1000, color='red')\n",
    "        else:\n",
    "            plt.scatter(pos[:, 0]*patch_size+int(patch_size/2), pos[:, 1]*patch_size+int(patch_size/2), s=500, zorder=1000, color='red')\n",
    "    else:\n",
    "        mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
    "        mask[index] = True\n",
    "        plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
    "        plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
    "    plt.axis('off')\n",
    "    plt.gca().invert_yaxis()\n",
    "    if img is not None:\n",
    "        im = plt.imread(img)\n",
    "        plt.imshow(im, alpha=0.5)\n",
    "    if fig_text is not None:\n",
    "        plt.figtext(0.5, 0.1, fig_text, ha=\"center\", fontsize=18, bbox={\"facecolor\":box_color, \"alpha\":0.5, \"pad\":5} )\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return fig\n",
    "    \n",
    "def visualize_projection(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_grid(pos, color):\n",
    "    color = color.detach().cpu().numpy()\n",
    "    pos = pos.detach().cpu().numpy()\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_graph(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
    "                         node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d25aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_to_graph(csv, class_codes, num_feat=(1, 513), compute_edges=False, weight_func=cosine_similarity, radius=1.5, remove_empty=False, weights_exposure=False):\n",
    "    df = pd.read_csv(csv)\n",
    "    feat_arr = np.array(df.iloc[:, num_feat[0]:num_feat[1]])\n",
    "    pos_arr = np.array(df.iloc[:, -3:-1])\n",
    "    label_arr = np.array(class_codes.get(df.iloc[0, -1]))\n",
    "\n",
    "    # if remove_empty and os.path.splitext(os.path.basename(csv))[0]:\n",
    "    #     empty_idx = np.where( np.sum(feat_arr, 1) == 0)[0]\n",
    "    #     feat_arr[empty_idx, :] = 0\n",
    "    #     if len(empty_idx)>0: pos_arr[empty_idx, :] = pos_arr[empty_idx[0], :]\n",
    "\n",
    "    data = Data(x=torch.tensor(feat_arr, dtype=torch.float), pos=torch.tensor(pos_arr, dtype=torch.long), y=torch.tensor(label_arr, dtype=torch.long))\n",
    "    radius_graph = RadiusGraph(radius, loop=True) # 1.5 or 2\n",
    "    data = radius_graph(data) \n",
    "    if compute_edges:\n",
    "        weights = []\n",
    "        for i in range(data.edge_index.shape[1]):\n",
    "            edge = data.edge_index[:, i]\n",
    "            weight = weight_func( data.x[edge[0]].view(1, -1), data.x[edge[1]].view(1, -1) )\n",
    "            weights.append(weight)\n",
    "        if weights_exposure: weights = exposure.rescale_intensity( np.vstack(weights).squeeze(), in_range=(0.5, 1), out_range=(0, 1) )\n",
    "        weights = np.vstack(weights).squeeze()\n",
    "        data.edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    data.name = os.path.splitext(os.path.basename(csv))[0]\n",
    "    data.label = df.iloc[0, -1]\n",
    "    return data\n",
    "\n",
    "def compute_dataset(embedder, path, class_codes, ext='png', opt_folder=False):\n",
    "    classes = glob(join(path, '*'))\n",
    "    # print(classes) \n",
    "    data_list = [] # pos, feats, node labels, node_mask, graph label\n",
    "    # return None\n",
    "    for idx, c in enumerate(classes):\n",
    "        class_name = basename(c)\n",
    "        if opt_folder:\n",
    "            regions = glob(join(path, class_name, '*', '*'))\n",
    "        else:\n",
    "            regions = glob(join(path, class_name, '*'))\n",
    "        regions = [x for x in regions if os.path.isdir(x)]\n",
    "        for region in tqdm(regions):\n",
    "            try:\n",
    "                pos_list = []\n",
    "                feat_list = []\n",
    "                node_list = []\n",
    "                mask_list = []\n",
    "                patches = glob(join(region, '*.'+ext))\n",
    "                for patch in patches:\n",
    "                    ### patch pos -> x, y\n",
    "                    patch_data = img_as_float(io.imread(patch))\n",
    "\n",
    "                    # feat = np.mean(patch_data)\n",
    "                    with torch.no_grad():\n",
    "                        patch_tensor = torch.FloatTensor(patch_data.transpose(2, 0, 1))[None, :].cuda()\n",
    "                        feat_tensor = embedder(patch_tensor)\n",
    "                        feat = feat_tensor.detach().cpu().numpy().squeeze()\n",
    "                    masked = False\n",
    "                    node_label = np.nan\n",
    "\n",
    "                    x = int(splitext(basename(patch))[0].split('_')[0])\n",
    "                    y = int(splitext(basename(patch))[0].split('_')[1])\n",
    "                    pos_list.append((x, y))\n",
    "                    feat_list.append(feat)  \n",
    "                    node_list.append(node_label)\n",
    "                    mask_list.append(masked)\n",
    "                pos_arr = np.vstack(pos_list)\n",
    "                feat_arr = np.vstack(feat_list)\n",
    "                node_arr = np.vstack(node_list)\n",
    "                mask_arr = np.vstack(mask_list)\n",
    "                graph_label = np.array(class_codes.get(class_name))\n",
    "                graph_name = region\n",
    "                data_list.append((pos_arr, feat_arr, node_arr, mask_arr, graph_label, graph_name))\n",
    "            except:\n",
    "                print(region)\n",
    "    return data_list\n",
    "\n",
    "def sample_weights(graph_train):\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    _, counts = np.unique(n_sample, return_counts=True)\n",
    "    counts = counts.max() / (10e-3+counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd710312-a6b0-4d71-aff4-bd597820fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_codes = {\n",
    "    'normal': (0, 0),\n",
    "    'pancreatitis': (0, 1),\n",
    "    'pdac': (1, 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a669b6",
   "metadata": {},
   "source": [
    "### Compute representations of patches of every ROI, save them as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path = 'data_bags/TMA' # bags of patches\n",
    "patch_ext = 'png' # extension of patch file\n",
    "out_path = 'dataframes/TMA' # feature files output directory\n",
    "resnet = models.resnet18(pretrained=True) # embedder\n",
    "resnet.fc = nn.Identity() # remove fc layer\n",
    "resnet = resnet.cuda()\n",
    "resnet.eval()\n",
    "data_list = compute_dataset(resnet, bag_path, distributed_codes, ext='png')\n",
    "for data in tqdm(data_list):\n",
    "    pos = data[0]\n",
    "    feat = data[1]\n",
    "    label = data[5].split(os.sep)[-2]\n",
    "    name = os.sep.join(data[5].split(os.sep)[-2:])\n",
    "    save_name = os.path.join(out_path, name)\n",
    "    os.makedirs(os.sep.join(save_name.split(os.sep)[:-1]), exist_ok=True)\n",
    "    df = pd.DataFrame(data=feat)\n",
    "    df = df.assign(pos_x=pd.Series(pos[:, 0]).values)\n",
    "    df = df.assign(pos_y=pd.Series(pos[:, 1]).values)\n",
    "    df = df.assign(label=pd.Series([label]*feat.shape[0]).values)\n",
    "    df.to_csv(save_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2144da",
   "metadata": {},
   "source": [
    "### Read representations of ROIs from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'dataframes/TMA'  # directoyr of feature files\n",
    "num_feat = (1, 513) # 512x1 vector\n",
    "csvs = glob(os.path.join(csv_path, '*', '*.csv'))\n",
    "train_graph_list = []\n",
    "fov_normal = []\n",
    "fov_pancreatitis = []\n",
    "fov_pdac = []\n",
    "for csv in tqdm(csvs):\n",
    "    data = read_dataset_to_graph(csv, distributed_codes, num_feat, compute_edges=True, weight_func=cosine_similarity, radius=1.5, weights_exposure=False)\n",
    "    train_graph_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27182867",
   "metadata": {},
   "source": [
    "### Core level separation for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fold = 4\n",
    "while(True):\n",
    "    weight_mean = max(sample_weights(train_graph_list))\n",
    "    shuffle(train_graph_list)\n",
    "    cores_per_chunk = int(len(train_graph_list)/num_fold)\n",
    "    chunks_data = [train_graph_list[i*cores_per_chunk:(i+1)*cores_per_chunk] for i in range(num_fold)]\n",
    "    weights = []\n",
    "    for chunk_data in chunks_data:\n",
    "        weights.append(sample_weights(chunk_data))\n",
    "    max_weight = max([max(i) for i in weights])\n",
    "    if max_weight<weight_mean*1.2: break\n",
    "fig = visualize_points(train_graph_list[0].pos, edge_index=train_graph_list[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00191a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_roc(labels, predictions, pos_label=1):\n",
    "    num_class = labels.shape[1]\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    thresholds = []\n",
    "    thresholds_optimal = []\n",
    "    aucs = []\n",
    "    if len(predictions.shape)==1:\n",
    "        predictions = predictions[:, None]\n",
    "    for c in range(0, num_class):\n",
    "        label = labels[:, c]\n",
    "        prediction = predictions[:, c]\n",
    "        fpr, tpr, threshold = roc_curve(label, prediction, pos_label=pos_label)\n",
    "        fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n",
    "        c_auc = roc_auc_score(label, prediction)\n",
    "        aucs.append(c_auc)\n",
    "        thresholds.append(threshold)\n",
    "        thresholds_optimal.append(threshold_optimal)\n",
    "    return aucs, thresholds, thresholds_optimal\n",
    "\n",
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]\n",
    "\n",
    "def accuracy_from_thresh(labels, predictions, thresh, priority_func=None): # numpy array, 2D, 2D, 1D\n",
    "    labels = copy.deepcopy(labels)\n",
    "    predictions = copy.deepcopy(predictions)\n",
    "    num_class = labels.shape[1]\n",
    "    if num_class==1:\n",
    "        class_prediction_bag = copy.deepcopy(predictions)\n",
    "        class_prediction_bag[predictions>=thresh[0]] = 1\n",
    "        class_prediction_bag[predictions<thresh[0]] = 0\n",
    "        predictions = class_prediction_bag\n",
    "    else:        \n",
    "        for i in range(num_class):\n",
    "            class_prediction_bag = copy.deepcopy(predictions[:, i])\n",
    "            class_prediction_bag[predictions[:, i]>=thresh[i]] = 1\n",
    "            class_prediction_bag[predictions[:, i]<thresh[i]] = 0\n",
    "            predictions[:, i] = class_prediction_bag\n",
    "    if priority_func is not None:\n",
    "        predictions = priority_func(predictions)\n",
    "        labels = priority_func(labels)\n",
    "        # print(predictions)\n",
    "        # print(labels)\n",
    "    # zeros = np.zeros((labels.shape[0], 1))\n",
    "    # labels = np.argmax(np.concatenate((zeros, labels), 1), 1)\n",
    "    # predictions = np.argmax(np.concatenate((zeros, predictions), 1), 1)\n",
    "    acc = balanced_accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2db9d53-7fce-43b2-adb2-48ca6a1295fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for data in loader:  \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(\n",
    "            data.x.float().cuda(), \n",
    "            edge_index=data.edge_index.cuda(), \n",
    "            batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(), \n",
    "            edge_weight= data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "            )\n",
    "        loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        loss_avg += loss.item()\n",
    "        count += 1\n",
    "    return loss_avg / count\n",
    "\n",
    "def test(loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        loss_avg = 0\n",
    "        for data in loader:   \n",
    "            out = model(\n",
    "                data.x.float().cuda(), \n",
    "                edge_index=data.edge_index.long().cuda(), \n",
    "                batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(),\n",
    "                edge_weight= data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "                )  \n",
    "            loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "            preds.append(torch.sigmoid(out).cpu().numpy().squeeze())\n",
    "            labels.append(data.y.numpy().squeeze())\n",
    "            loss_avg += loss.item()\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        aucs, thresholds, thresholds_optimal = multi_label_roc(labels, preds)\n",
    "        def priority_func(pred):\n",
    "            codes = []\n",
    "            for x in pred:\n",
    "                if x[0]==1: code = 2\n",
    "                if x[0]==0 and x[1]==0: code = 0\n",
    "                if x[0]==0 and x[1]==1: code = 1\n",
    "                codes.append(code)\n",
    "            return codes\n",
    "        # priority_func = None\n",
    "        acc, f1 = accuracy_from_thresh(labels, preds, thresholds_optimal, priority_func)\n",
    "    return loss_avg / len(loader), acc, f1, aucs, thresholds_optimal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026da89",
   "metadata": {},
   "source": [
    "### Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bfcff-147b-49dc-9a57-05963a3cca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "weight_decay = 0.0001\n",
    "weights_name = 'GCN_TMA_Distributed'\n",
    "weights_path = 'weights_cv'\n",
    "os.makedirs('weights_cv', exist_ok=True)\n",
    "model = GraphNet(\n",
    "        in_channels=512, \n",
    "        out_channels=2, \n",
    "        hidden_channels=512, \n",
    "        gcn_layer='GCN', \n",
    "        graph_pool='mean', \n",
    "        drop_p0=0.0,\n",
    "        drop_p1=0.25, \n",
    "        preprocess=Preprocess(in_channels=512, hidden_channels=512, out_channels=512, n_layers=0),\n",
    "        )\n",
    "lr = 0.0002\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbca3b",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af793c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(weights_path, 'init_distributed.pth'))\n",
    "for i in range(num_fold):\n",
    "    model.load_state_dict(torch.load(os.path.join(weights_path, 'init_distributed.pth')))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 0.00001)\n",
    "    optimal_score = 0\n",
    "    print(f'Running cross-validation fold: {i:02d}')\n",
    "    graph_train  = []\n",
    "    graph_test = []\n",
    "    for k in range(num_fold):\n",
    "        if k != i: graph_train += chunks_data[k]\n",
    "        else: graph_test = chunks_data[k]\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    counts = np.count_nonzero(n_sample, axis=0)\n",
    "    counts = counts.max() / counts\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(counts)).cuda()\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss().cuda()    \n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        shuffle(graph_train)\n",
    "        train(graph_train)\n",
    "        scheduler.step()    \n",
    "        train_loss, train_acc, _, _, _ = test(graph_train)\n",
    "        test_loss, test_acc, avg_prc, aucs, thresh = test(graph_test)\n",
    "        log_msg = f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test f1: {avg_prc:.4f}' + ', AUC: ' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs))\n",
    "        print(log_msg)\n",
    "        sum_metrics = (avg_prc+test_acc+sum(aucs))/3\n",
    "        if sum_metrics > optimal_score:\n",
    "            optimal_score = sum_metrics\n",
    "            os.makedirs(weights_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(weights_path, f'{weights_name}_fold{i:02d}.pth'))\n",
    "            print('Saved model!')\n",
    "            print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh)))\n",
    "            with open(os.path.join(weights_path, f'{weights_name}_fold{i:02d}.txt'), \"w\") as text_file:\n",
    "                print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh))+'|||'+log_msg, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9168",
   "metadata": {},
   "source": [
    "### Read result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10904f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de88db708fd7569f2666ff37f921dc0f88a8459546879a55f00d65d0006c4c3e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
